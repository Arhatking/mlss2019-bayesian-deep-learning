{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLSS2019: Bayesian Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will learn what basic building blocks are needed\n",
    "to endow (deep) neural networks with uncertainty estimates, and how\n",
    "this can be used in active learning or expert-in-the-loop pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plan of the tutorial\n",
    "1. [Setup and imports](#Setup-and-imports)\n",
    "2. [Bayesian Active Learning with images](#Bayesian-Active-Learning-with-images)\n",
    "   1. [the Acquisition Function](#the-Acquisition-Function)\n",
    "   2. [Actively learning MNIST](#Actively-Learning-MNIST)\n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(note)**\n",
    "* to view documentation on something  type in `something?` (with one question mark)\n",
    "* to view code of something type in `something??` (with two question marks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we import necessary modules and functions and\n",
    "define the computational device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we install some boilerplate service code for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --upgrade git+https://github.com/ivannz/mlss2019-bayesian-deep-learning.git@alternative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, numpy for computing, matplotlib for plotting and tqdm for progress bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For deep learning stuff will be using [pytorch](https://pytorch.org/).\n",
    "\n",
    "If you are unfamiliar with it, it is basically like `numpy` with autograd,\n",
    "native GPU support, and tools for building training and serializing models.\n",
    "<!-- (and with `axis` argument replaced with `dim` :) -->\n",
    "\n",
    "There are good introductory tutorials on `pytorch`, like this\n",
    "[one](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need some functionality from scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import the boilerplate code.\n",
    "\n",
    "* a procedure that implements a minibatch SGD **fit** loop\n",
    "* a function, that **evaluates** the model on the provided dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlss2019bdl import fit, predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm to sample a random function is:\n",
    "* for $b = 1... B$ do:\n",
    "\n",
    "  1. draw an independent realization $f_b\\colon \\mathcal{X} \\to \\mathcal{Y}$\n",
    "  with from the process $\\{f_\\omega\\}_{\\omega \\sim q(\\omega)}$\n",
    "  2. get $\\hat{y}_{bi} = f_b(\\tilde{x}_i)$ for $i=1 .. m$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlss2019bdl.bdl import freeze, unfreeze\n",
    "\n",
    "def sample_function(model, dataset, n_samples=1, verbose=False):\n",
    "    \"\"\"Draw a realization of a random function.\"\"\"\n",
    "    outputs = []\n",
    "    for _ in tqdm.tqdm(range(n_samples), disable=not verbose):\n",
    "        freeze(model)\n",
    "\n",
    "        outputs.append(predict(model, dataset))\n",
    "\n",
    "    unfreeze(model)\n",
    "\n",
    "    return torch.stack(outputs, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Active Learning with images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data labelling is costly and time consuming\n",
    "* unlabeled instances are essentially free\n",
    "\n",
    "**Goal** Achieve high performance with fewer labels by\n",
    "identifying the best instances to learn from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essential blocks of active learning:\n",
    "\n",
    "* a **model** $m$ capable of quantifying uncertainty (preferably a Bayesian model)\n",
    "* an **acquisition function** $a\\colon \\mathcal{M} \\times \\mathcal{X}^* \\to \\mathbb{R}$\n",
    "  that for any finite set of inputs $S\\subset \\mathcal{X}$ quantifies their usefulness\n",
    "  to the model $m\\in \\mathcal{M}$\n",
    "* a labelling **oracle**, e.g. a human expert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main loop of active learning:\n",
    "\n",
    "1. fit $m$ on $\\mathcal{S}_{\\mathrm{labelled}}$\n",
    "\n",
    "2. get exact (or approximate) $$\n",
    "    \\mathcal{S}^* \\in \\arg \\max\\limits_{S \\subseteq \\mathcal{S}_\\mathrm{unlabelled}}\n",
    "        a(m, S)\n",
    "$$ satisfying **budget constraints** and **without** access to targets\n",
    "(constraints, like $\\lvert S \\rvert \\leq \\ell$ or other economically motivated ones).\n",
    "\n",
    "3. request the **oracle** to provide labels for each $x\\in \\mathcal{S}^*$\n",
    "\n",
    "4. update $\n",
    "\\mathcal{S}_{\\mathrm{labelled}}\n",
    "    \\leftarrow \\mathcal{S}^*\n",
    "        \\cup \\mathcal{S}_{\\mathrm{labelled}}\n",
    "$ and goto 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have a Bayesian model that can be used to reason\n",
    "about uncertainty, so let's focus on the acquisition function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the Acquisition Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many acquisition criteria (borrowed from [Gal17a](http://proceedings.mlr.press/v70/gal17a.html)):\n",
    "* Classification\n",
    "  * Max entropy (plain uncertainty)\n",
    "  * Maximal information about parameters and predictions (mutual information)\n",
    "  * Variance ratios\n",
    "  * Mean standard deviation\n",
    "  * **BALD**\n",
    "* Regression\n",
    "  * predictive variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BALD** (Bayesian Active Learning by Disagreement) acquisition\n",
    "criterion is based on the posterior mutual information between model's\n",
    "predictions $y_x$ at some point $x$ and model's parameters $\\omega$:\n",
    "\n",
    "$$\\begin{align}\n",
    "    a(m, S)\n",
    "        &= \\sum_{x\\in S} a(m, \\{x\\})\n",
    "        \\\\\n",
    "    a(m, \\{x\\})\n",
    "        &= \\mathbb{I}(y_x; \\omega \\mid x, m, D)\n",
    "\\end{align}\n",
    "    \\,, \\tag{bald} $$\n",
    "\n",
    "with the [**Mutual Information**](https://en.wikipedia.org/wiki/Mutual_information#Relation_to_Kullback%E2%80%93Leibler_divergence)\n",
    "(**MI**)\n",
    "$$\n",
    "    \\mathbb{I}(y_x; \\omega \\mid x, m, D)\n",
    "        = \\mathbb{H}\\bigl(\n",
    "            \\mathbb{E}_{\\omega \\sim q(\\omega\\mid m, D)}\n",
    "                p(y_x \\,\\mid\\, x, \\omega, m, D)\n",
    "        \\bigr)\n",
    "        - \\mathbb{E}_{\\omega \\sim q(\\omega\\mid m, D)}\n",
    "            \\mathbb{H}\\bigl(\n",
    "                p(y_x \\,\\mid\\, x, \\omega, m, D)\n",
    "            \\bigr)\n",
    "    \\,, \\tag{mi} $$\n",
    "\n",
    "and the [(differential) **entropy**](https://en.wikipedia.org/wiki/Differential_entropy#Differential_entropies_for_various_distributions)\n",
    "(all densities and/or probability mass functions can be conditional):\n",
    "\n",
    "$$\n",
    "    \\mathbb{H}(p(y))\n",
    "        = - \\mathbb{E}_{y\\sim p} \\log p(y)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of the exact formula for **MI** we shall use its **Monte Carlo** (**MC**)\n",
    "approximation, since the expectations are analytically or numerically\n",
    "tractable only in simple low dimensional cases.\n",
    "<!-- probability integrals are still integrals -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider an iid sample $\\mathcal{W} = (\\omega_b)_{b=1}^B \\sim q(\\omega \\mid m, D)$\n",
    "of size $B$. The **MC** approximation of the mutual information is\n",
    "\n",
    "$$\n",
    "    \\mathbb{I}_\\mathrm{MC}(y_x; \\omega \\mid x, m, D)\n",
    "        = \\mathbb{H}\\bigl(\n",
    "            \\hat{\\mathbb{E}}_{\\omega \\sim \\mathcal{W}}\n",
    "                p(y_x \\,\\mid\\, x, \\omega, m, D)\n",
    "        \\bigr)\n",
    "        - \\hat{\\mathbb{E}}_{\\omega \\sim \\mathcal{W}}\n",
    "            \\mathbb{H}\\bigl(\n",
    "                p(y_x \\,\\mid\\, x, \\omega, m, D)\n",
    "            \\bigr)\n",
    "    \\,, \\tag{mi-mc} $$\n",
    "\n",
    "where $\\hat{\\mathbb{E}}_{\\omega \\sim \\mathcal{W}} h(\\omega) = \\tfrac1B \\sum_j h(\\omega_j)$\n",
    "denotes the expectation with respect to the empirical probability measure induced\n",
    "by the sample $\\mathcal{W}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (task) implementing entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For categorical (discrete) random variables $y \\sim \\mathcal{Cat}(\\mathbf{p})$,\n",
    "$\\mathbf{p} \\in \\{ \\mu \\in [0, 1]^d \\colon \\sum_k \\mu_k = 1\\}$, the entropy is\n",
    "\n",
    "$$\n",
    "    \\mathbb{H}(p(y))\n",
    "        = - \\mathbb{E}_{y\\sim p(y)} \\log p(y)\n",
    "        = - \\sum_k p_k \\log p_k\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(note)** although in calculus $0 \\cdot \\log 0 = 0$ (because\n",
    "$\\lim_{p\\downarrow 0} p \\cdot \\log p = 0$), in floating point\n",
    "arithmetic $0 \\cdot \\log 0 = \\mathrm{NaN}$. So you need to add\n",
    "some **really tiny float number** to the argument of $\\log$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(proba):\n",
    "    \"\"\"Compute the entropy along the last dimension.\"\"\"\n",
    "\n",
    "    ## Exercise: get the entropy of a tensor with distributions\n",
    "    #  along the last axis.\n",
    "\n",
    "    return - torch.kl_div(torch.tensor(0.).to(proba), proba).sum(dim=-1)\n",
    "    return - torch.sum(proba * torch.log(proba + 1e-20), dim=-1)\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (task) implementing mutual information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a tensor $p_{bik}$ of probabilities $p(y_{x_i}=k \\mid x_i, \\omega_b, m, D)$\n",
    "with $\\omega_b \\sim q(\\omega \\mid m, D)$.\n",
    "\n",
    "Let's implement a procedure that computes the **MC** estimate \n",
    "of the posterior predictive distribution\n",
    "\n",
    "$$\n",
    "\\hat{p}(y_x\\mid x, m, D)\n",
    "    = \\hat{\\mathbb{E}}_{\\omega \\sim \\mathcal{W}}\n",
    "        \\,p(y_x \\mid x, \\omega, m, D)\n",
    "    \\,, $$\n",
    "\n",
    "its **entropy** $\n",
    "    \\mathbb{H}\\bigl(\\hat{p}(y\\mid x, m, D)\\bigr)\n",
    "$ and **mutual information** $\n",
    "    \\mathbb{I}_\\mathrm{MC}(y_x ; \\omega\\mid x, m, D)\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_information(proba):\n",
    "    ## Exercise: compute a Monte Carlo estimator of the predictive\n",
    "    ##   distribution, its entropy and MI `H E_w p(., w) - E_w H p(., w)`\n",
    "\n",
    "    proba_avg = proba.mean(dim=0)\n",
    "\n",
    "    entropy_expected = entropy(proba_avg)\n",
    "    expected_entropy = entropy(proba).mean(dim=0)\n",
    "\n",
    "    mut_info = entropy_expected - expected_entropy\n",
    "\n",
    "    pass\n",
    "\n",
    "    return proba_avg, entropy_expected, mut_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (task) implementing BALD acqustion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The acquisition function that we will implement takes in the\n",
    "sample mutual information and returns the indices of selected\n",
    "points.\n",
    "\n",
    "\n",
    "\n",
    "Note that $a(m, S)$ is additively separable, i.e. equals $\\sum_{x\\in S} a(m, \\{x\\})$.\n",
    "This implies that\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\max_{S \\subseteq \\mathcal{S}_\\mathrm{unlabelled}} a(m, S)\n",
    "        &= \\max_{z \\in \\mathcal{S}_\\mathrm{unlabelled}}\n",
    "            \\max_{F \\in \\mathcal{S}_\\mathrm{unlabelled} \\setminus \\{z\\}}\n",
    "            \\sum_{x\\in F \\cup \\{x\\}} a(m, \\{x\\})\n",
    "        \\\\\n",
    "        &= \\max_{z \\in \\mathcal{S}_\\mathrm{unlabelled}}\n",
    "            a(m, \\{z\\})\n",
    "            + \\max_{F \\in \\mathcal{S}_\\mathrm{unlabelled} \\setminus \\{z\\}}\n",
    "                \\sum_{x\\in F} a(m, \\{x\\})\n",
    "\\end{align}\n",
    "    \\,. $$\n",
    "\n",
    "Therefore selecting the $\\ell$ `most interesting` points from $\\mathcal{S}_\\mathrm{unlabelled}$\n",
    "is trivial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acq_bald(mutual_info, n_points=10):\n",
    "    ## Exercise: implement the acquisition\n",
    "\n",
    "    indices = mutual_info.argsort()\n",
    "\n",
    "    return indices[-n_points:]\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(note)** A drawback of the `pointwise` top-$\\ell$ procedure above is\n",
    "that, although it acquires individually informative instances, altogether\n",
    "they might end up **being** `jointly poorly informative`. This can be\n",
    "corrected if we would seek the highest mutual information among finite\n",
    "sets $S \\subseteq \\mathcal{S}_\\mathrm{unlabelled}$ of size $\\ell$.\n",
    "Such acquisition function is called **batch-BALD**\n",
    "([Kirsch et al.; 2019](https://arxiv.org/abs/1906.08158.pdf)):\n",
    "\n",
    "$$\\begin{align}\n",
    "    a(m, S)\n",
    "        &= \\mathbb{I}\\bigl((y_x)_{x\\in S}; \\omega \\mid S, m \\bigr)\n",
    "        = \\mathbb{H} \\bigl(\n",
    "            \\mathbb{E}_{\\omega \\sim q(\\omega\\mid m)} p\\bigl((y_x)_{x\\in S}\\mid S, \\omega, m \\bigr)\n",
    "        \\bigr)\n",
    "        - \\mathbb{E}_{\\omega \\sim q(\\omega\\mid m)} H\\bigl(\n",
    "            p\\bigl((y_x)_{x\\in S}\\mid S, \\omega, m \\bigr)\n",
    "        \\bigr)\n",
    "\\end{align}\n",
    "    \\,. \\tag{batch-bald} $$\n",
    "\n",
    "This criterion requires exponentially large number of computations and\n",
    "memory, however there are working solutions like random sampling of subsets\n",
    "$\\mathcal{S}$ of size $\\ell$ from $\\mathcal{S}_\\mathrm{unlabelled}$ or\n",
    "greedy maximization of this *submodular* criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Kirsch, A., van Amersfoort, J., & Gal, Y. (2019). BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning. arXiv preprint [arXiv:1906.08158](https://arxiv.org/abs/1906.08158.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (task*) Unbiased estimator of entropy and mutual information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first term in the **MC** estimate of the mutual information is the\n",
    "so-called **plug-in** estimator of the entropy:\n",
    "\n",
    "$$\n",
    "    \\hat{H}\n",
    "        = \\mathbb{H}(\\hat{p}) = - \\sum_k \\hat{p}_k \\log \\hat{p}_k\n",
    "    \\,, $$\n",
    "\n",
    "where $\\hat{p}_k = \\tfrac1B \\sum_b p_{bk}$ is the full sample estimator\n",
    "of the probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is known that this plug-in estimate is biased\n",
    "(see [blog: Nowozin, 2015](http://www.nowozin.net/sebastian/blog/estimating-discrete-entropy-part-1.html)\n",
    "and references therein, also this [notebook](https://colab.research.google.com/drive/1z9ZDNM6NFmuFnU28d8UO0Qymbd2LiNJW)). <!--($\\log$ + Jensen)-->\n",
    "In order to correct for small-sample bias we can use\n",
    "[jackknife resampling](https://en.wikipedia.org/wiki/Jackknife_resampling).\n",
    "It derives an estimate of the finite sample bias from the leave-one-out\n",
    "estimators of the entropy and is relatively computationally cheap\n",
    "(see [blog: Nowozin, 2015](http://www.nowozin.net/sebastian/blog/estimating-discrete-entropy-part-2.html),\n",
    "[Miller, R. G. (1974)](http://www.math.ntu.edu.tw/~hchen/teaching/LargeSample/references/Miller74jackknife.pdf) and these [notes](http://people.bu.edu/aimcinto/jackknife.pdf)).\n",
    "\n",
    "The jackknife correction of a plug-in estimator $\\mathbb{H}(\\cdot)$\n",
    "is computed thus: given a sample $(p_b)_{b=1}^B$ with $p_b$ -- discrete distribution on $1..K$\n",
    "* for each $b=1.. B$\n",
    "  * get the leave-one-out estimator: $\\hat{p}_k^{-b} = \\tfrac1{B-1} \\sum_{j\\neq b} p_{jk}$\n",
    "  * compute the plug-in entropy estimator: $\\hat{H}_{-b} = \\mathbb{H}(\\hat{p}^{-b})$\n",
    "* then compute the bias-corrected entropy estimator $\n",
    "\\hat{H}_J\n",
    "    = \\hat{H} + (B - 1) \\bigl\\{\n",
    "        \\hat{H} - \\tfrac1B \\sum_b \\hat{H}^{-b}\n",
    "    \\bigr\\}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(note)** when we knock the $i$-th data point out of the sample mean\n",
    "$\\mu = \\tfrac1n \\sum_i x_i$ and recompute the mean $\\mu_{-i}$ we get\n",
    "the following relation\n",
    "$$ \\mu_{-i}\n",
    "    = \\frac1{n-1} \\sum_{j\\neq i} x_j\n",
    "    = \\frac{n}{n-1} \\mu - \\tfrac1{n-1} x_i\n",
    "    = \\mu + \\frac{\\mu - x_i}{n-1}\n",
    "    \\,. $$\n",
    "This makes it possible to quickly compute leave-one-out estimators of\n",
    "discrete probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    def mutual_information(proba):\n",
    "        ## Exercise: MC estimate of the predictive distribution, entropy and MI\n",
    "        ##  mutual information `H E_w p(., w) - E_w H p(., w)` with jackknife\n",
    "        ##  correction.\n",
    "\n",
    "        proba_avg = proba.mean(dim=0)\n",
    "\n",
    "        # plug-in estimate of entropy\n",
    "        entropy_expected = entropy(proba_avg)\n",
    "\n",
    "        # jackknife correction\n",
    "        proba_loo = proba_avg + (proba_avg - proba) / (len(proba) - 1)\n",
    "\n",
    "        expected_entropy_loo = entropy(proba_loo).mean(dim=0)\n",
    "        entropy_expected += (len(proba) - 1) * (entropy_expected - expected_entropy_loo)\n",
    "\n",
    "        # expected entropy is unbiased\n",
    "        expected_entropy = entropy(proba).mean(dim=0)\n",
    "\n",
    "        mut_info = entropy_expected - expected_entropy\n",
    "\n",
    "        pass\n",
    "\n",
    "        return proba_avg, entropy_expected, mut_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actively Learning MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will partially replicate figure 1. in [Gat et al. (2017): p. 4](http://proceedings.mlr.press/v70/gal17a.html),\n",
    "\n",
    "> Gal, Y., Islam, R. & Ghahramani, Z.. (2017). Deep Bayesian Active Learning with Image Data. Proceedings of the 34th International Conference on Machine Learning, in [PMLR 70:1183-1192](http://proceedings.mlr.press/v70/gal17a.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the datasets from the `train` part of [MNIST](http://yann.lecun.com/exdb/mnist/):\n",
    "* ($\\mathcal{S}_\\mathrm{train}$) initial **training**:\n",
    "  **empty** -- learn from scratch\n",
    "  <strike>21 images, purposefully highly imbalanced classes (even absent ones)</strike>\n",
    "* ($\\mathcal{S}_\\mathrm{valid}$) our **validation**:\n",
    "  $5000$ images, stratified\n",
    "* ($\\mathcal{S}_\\mathrm{pool}$) acquisition **pool**:\n",
    "  all remaining images\n",
    "\n",
    "The true test sample of MNIST is in $\\mathcal{S}_\\mathrm{test}$ -- we\n",
    "will use it to evaluate the final performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlss2019bdl.dataset import get_dataset\n",
    "\n",
    "S_train, S_pool, S_valid, S_test = get_dataset(\n",
    "    n_train=0, n_valid=5000, name=\"MNIST\",\n",
    "    random_state=722_257_201, path=\"./data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(note)** We may just as well use [Kuzushiji-MNIST](https://github.com/rois-codh/kmnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a function to plot images in a small dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlss2019bdl.flex import plot\n",
    "\n",
    "def display(dataset, title=None, show_balance=True, figsize=None):\n",
    "    images, targets = dataset.tensors\n",
    "    if not show_balance:\n",
    "        balance = \"\"\n",
    "\n",
    "    else:\n",
    "        body = [f\"{n:2d}\" if n > 0 else \" *\"\n",
    "                for n in label_counts(targets)]\n",
    "        balance = \"(freq) [ \" + ' '.join(body) + \" ]\"\n",
    "\n",
    "    # a canvas\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "    # show the images\n",
    "    plot(ax, images, cmap=plt.cm.bone)\n",
    "\n",
    "    # produce a title\n",
    "    title = \"\" if title is None else title\n",
    "    title = title + (\" \" if title else \"\")\n",
    "    ax.set_title(f\"{title}{balance}\")\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def label_counts(labels, n_labels=10):\n",
    "    return np.bincount(labels.numpy(), minlength=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(S_train, title=\"Train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Necessary components for the loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to be able to manipulate the datasets for the **main active learning**\n",
    "loop. We begin by implementing the following primitives:\n",
    "* `take` collect the instances at the specified indices into a **new dataset** (object)\n",
    "* `append` add one dataset to another\n",
    "* `delete` drops the instances at the specified locations form the copy of the **dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlss2019bdl.dataset import take, delete, append"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the **main active learning** loop, besides manipulating the datasets,\n",
    "we shall also need a function to **predict and acquire** and evaluate\n",
    "holdout **performance**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(model, dataset, n_samples=1):\n",
    "    logits = sample_function(model, dataset, n_samples=n_samples)\n",
    "\n",
    "    # logit-scores should be transformed into a proper distribution\n",
    "    return F.softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquire(model, dataset, n_points=10, n_samples=1):\n",
    "    proba = predict_proba(model, dataset, n_samples=n_samples)\n",
    "\n",
    "    _, _, mutual_info = mutual_information(proba)\n",
    "\n",
    "    return acq_bald(mutual_info, n_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataset, n_samples=1):\n",
    "    proba = predict_proba(model, dataset, n_samples=n_samples)\n",
    "    \n",
    "    proba_avg = proba.mean(dim=0)\n",
    "\n",
    "    predicted = proba_avg.argmax(dim=-1).numpy()\n",
    "    target = dataset.tensors[1].cpu().numpy()\n",
    "\n",
    "    return confusion_matrix(target, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like the `SimpleModel` class above in $1$d section,\n",
    "let's implement a simple deep convolutional network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear, Conv2d\n",
    "\n",
    "from mlss2019bdl.bdl import DropoutLinear, DropoutConv2d\n",
    "\n",
    "class CNNModel(torch.nn.Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_block = torch.nn.Sequential(\n",
    "            Conv2d(1, 20, 5, 1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool2d(2),\n",
    "\n",
    "            DropoutConv2d(20, 50, 5, 1, p=p),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.fc1 = DropoutLinear(4 * 4 * 50, 400, p=p)\n",
    "        self.out = DropoutLinear(400, 10, p=p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Take images and compute their class logits.\"\"\"\n",
    "        x = self.conv_block(input).flatten(1)\n",
    "\n",
    "        return self.out(F.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = CNNModel(p=0.5)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit the classifier that outputs raw logit scores, we typically\n",
    "normalize outputs via `F.log_softmax` and then feed into them into\n",
    "`F.nll_loss`, which computes the negative $\\log$-likelihood of a\n",
    "categorical distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this is less numerically stable then using `F.cross_entropy`,\n",
    "which is essentially` log_softmax + nll` fused into one stable operation.\n",
    "It is good practice to pay attention to numerical stability, especially\n",
    "when working with `float32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mlss2019bdl.bdl import penalties\n",
    "\n",
    "def cross_entropy(model, X, y):\n",
    "    return F.cross_entropy(model(X), y)  # + sum(penalties(model)) * 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (task) Implementing the active learning step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's code the core of the active learning loop:\n",
    "\n",
    "1. fit on **train**, then (optional) evaluate on **holdout**\n",
    "2. acquire from **pool**\n",
    "3. add to **train** (removing from **pool**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_acquire(model, S_train, S_pool,\n",
    "                         n_epochs=5, n_points=10, n_samples=11):\n",
    "    ## Exercise: implement the fit-acquire loop\n",
    "\n",
    "    # 1. fit on S_train using `cross_entropy`, set `weight_decay` to 1e-4\n",
    "    fit(model, S_train, criterion=cross_entropy, n_epochs=n_epochs, weight_decay=1e-4)\n",
    "\n",
    "    # 2. acquire new instances from S_pool\n",
    "    indices = acquire(model, S_pool, n_points=n_points, n_samples=n_samples)\n",
    "\n",
    "    # 3. query the pool for the chosen instances, then take-append-delete\n",
    "    S_requested = take(S_pool, indices)\n",
    "    S_train = append(S_train, S_requested)\n",
    "    S_pool = delete(S_pool, indices)\n",
    "\n",
    "    pass\n",
    "\n",
    "    return model, S_train, S_pool, S_requested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the fit-acquire procedure in a loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_epochs, n_samples = 5, 11\n",
    "n_active, n_points = 75, 10\n",
    "\n",
    "display(S_train, title=\"initial train\")\n",
    "\n",
    "scores = []\n",
    "balances = [label_counts(S_train.tensors[1])]\n",
    "for step in range(n_active):\n",
    "\n",
    "    model, S_train, S_pool, S_requested = fit_acquire(\n",
    "        model, S_train, S_pool, n_epochs=n_epochs,\n",
    "        n_points=n_points, n_samples=n_samples)\n",
    "\n",
    "    # (optional) track validation score\n",
    "    score_matrix = evaluate(model, S_valid, n_samples=n_samples)\n",
    "\n",
    "    # (optional) report accuracy and the statistics on the acquired batch\n",
    "    balances.append(label_counts(S_train.tensors[1]))\n",
    "    scores.append(score_matrix)\n",
    "\n",
    "    accuracy = score_matrix.diagonal().sum() / score_matrix.sum()\n",
    "    display(S_requested, title=f\"# {len(S_train)} (Acc.) {accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train of the final $\\mathcal{S}_\\mathrm{train}$ and evaluate the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balances = np.stack(balances, axis=0)\n",
    "\n",
    "fit(model, S_train, criterion=cross_entropy, n_epochs=n_epochs, weight_decay=1e-4)\n",
    "scores.append(evaluate(model, S_valid, n_samples=n_samples))\n",
    "\n",
    "scores = np.stack(scores, axis=0)\n",
    "\n",
    "display(S_train, title=\"final train\", figsize=(16, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the dynamics of the frequency of each class in $\\mathcal{S}_\\mathrm{train}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 7))\n",
    "\n",
    "lines = ax.plot(balances, lw=2)\n",
    "plt.legend(lines, list(range(10)), ncol=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dynamics of *one-versus-rest* precision / recall scores on\n",
    "$\\mathcal{S}_\\mathrm{valid}$. For binary classification:\n",
    "\n",
    "$$ \\begin{align}\n",
    "\\mathrm{Precision}\n",
    "    &= \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}}\n",
    "        \\approx \\mathbb{P}(y = 1 \\mid \\hat{y} = 1)\n",
    "    \\,, \\\\\n",
    "\\mathrm{Recall}\n",
    "    &= \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}}\n",
    "        \\approx \\mathbb{P}(\\hat{y} = 1 \\mid y = 1)\n",
    "    \\,.\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = scores.diagonal(axis1=-2, axis2=-1)\n",
    "fp, fn = scores.sum(axis=-2) - tp, scores.sum(axis=-1) - tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 7))\n",
    "\n",
    "lines = ax.plot(tp / (tp + fp), lw=2)\n",
    "ax.set_title(\"Precision (ovr)\")\n",
    "ax.legend(lines, list(range(10)), ncol=2);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 7))\n",
    "\n",
    "lines = ax.plot(tp / (tp + fn), lw=2)\n",
    "ax.set_title(\"Recall (ovr)\")\n",
    "ax.legend(lines, list(range(10)), ncol=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy as a function of active learning iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 7))\n",
    "\n",
    "ax.plot(tp.sum(-1) / scores.sum((-2, -1)),\n",
    "        label='Accuracy', lw=2)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volume of data used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"train : pool = {len(S_train)} : {len(S_pool)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the test set confusion matrix be the ultimate judge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_matrix = evaluate(model, S_test, n_samples=51)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True positives, and false positives / negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = score_matrix.diagonal(axis1=-2, axis2=-1)\n",
    "fp, fn = score_matrix.sum(axis=-2) - tp, score_matrix.sum(axis=-1) - tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"(accuracy) {tp.sum() / score_matrix.sum():.2%}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one-v-rest precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{l: f\"{p:.2%}\" for l, p in enumerate(tp / (tp + fp))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ovr recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{l: f\"{p:.2%}\" for l, p in enumerate(tp / (tp + fn))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
